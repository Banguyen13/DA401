{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training models on 3 small samples of cluster pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1=pd.read_parquet(\"old_new.parquet\")\n",
    "sample2=pd.read_parquet(\"hiphop_country.parquet\")\n",
    "sample3=pd.read_parquet(\"sad_bright.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old vs New\n",
    "X_audio1=sample1[sample1.columns[3:]].values\n",
    "y_audio1=sample1[\"Cluster\"].values\n",
    "X_lyrics1=np.vstack(old_new[\"lyrics_embeddings\"].values)\n",
    "y_lyrics1=old_new[\"Cluster\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_audio1 = kmeans.fit_predict(X_audio1)\n",
    "ari_audio1 = adjusted_rand_score(y_audio1, y_pred_audio1)\n",
    "print(\"Adjusted Rand Index (ARI) for audio embeddings:\", ari_audio1)\n",
    "y_pred_lyrics1=kmeans.fit_predict(X_lyrics1)\n",
    "ari_lyrics1 = adjusted_rand_score(y_lyrics1, y_pred_lyrics1)\n",
    "print(\"Adjusted Rand Index (ARI) for lyric embeddings:\", ari_lyrics1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluters generated by K-means are not inherent so we have to map it to the true labels to understand the performance of the model\n",
    "def map_clusters(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Matches predicted cluster labels to true labels using the Hungarian algorithm.\n",
    "    Returns a mapping dictionary where keys are predicted labels and values are the corresponding true labels.\n",
    "    \"\"\"\n",
    "    # Compute the contingency matrix (rows: true labels, columns: predicted labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    # Negate the matrix to convert maximization into minimization\n",
    "    cost_matrix = -cm\n",
    "    \n",
    "    # Apply the Hungarian algorithm (linear_sum_assignment)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    # Build the mapping: for each predicted label (col), assign the corresponding true label (row)\n",
    "    mapping = {pred: true for true, pred in zip(row_ind, col_ind)}\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of audio embeddings on old-new cluster pair\n",
    "true_labels_audio1 = np.array(y_audio1)       \n",
    "predicted_labels_audio1 = np.array(y_pred_audio1)\n",
    "mapping = map_clusters(true_labels_audio1, predicted_labels_audio1)\n",
    "print(\"Mapping of predicted to true labels:\", mapping)\n",
    "\n",
    "# Remap predicted labels based on the mapping:\n",
    "mapped_predicted_labels = np.array([mapping.get(label, label) for label in predicted_labels_audio1])\n",
    "print(\"Mapped predicted labels:\", mapped_predicted_labels)\n",
    "\n",
    "# Create confusion matrix with remapped labels\n",
    "cm_mapped = confusion_matrix(true_labels_audio1, mapped_predicted_labels)\n",
    "cm_df = pd.DataFrame(cm_mapped, \n",
    "                     index=[f\"True {i}\" for i in sorted(np.unique(true_labels_audio1))],\n",
    "                     columns=[f\"Pred {i}\" for i in sorted(np.unique(mapped_predicted_labels))])\n",
    "print(\"\\nConfusion Matrix (after mapping):\")\n",
    "print(cm_df)\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels_audio1, mapped_predicted_labels)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of lyrics embeddings on old-new cluster pair\n",
    "true_labels_lyrics1 = np.array(y_lyrics1)       \n",
    "predicted_labels_lyrics1 = np.array(y_pred_lyrics1)\n",
    "mapping = map_clusters(true_labels_lyrics1, predicted_labels_lyrics1)\n",
    "print(\"Mapping of predicted to true labels:\", mapping)\n",
    "\n",
    "# Remap predicted labels based on the mapping:\n",
    "mapped_predicted_labels = np.array([mapping.get(label, label) for label in predicted_labels_lyrics1])\n",
    "print(\"Mapped predicted labels:\", mapped_predicted_labels)\n",
    "\n",
    "# Create confusion matrix with remapped labels\n",
    "cm_mapped = confusion_matrix(true_labels_lyrics1, mapped_predicted_labels)\n",
    "cm_df = pd.DataFrame(cm_mapped, \n",
    "                     index=[f\"True {i}\" for i in sorted(np.unique(true_labels_lyrics1))],\n",
    "                     columns=[f\"Pred {i}\" for i in sorted(np.unique(mapped_predicted_labels))])\n",
    "print(\"\\nConfusion Matrix (after mapping):\")\n",
    "print(cm_df)\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels_lyrics1, mapped_predicted_labels)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiphop vs Pop Rock\n",
    "X_audio2=sample2[sample2.columns[3:]].values\n",
    "y_audio2=sample2[\"Cluster\"].values\n",
    "X_lyrics2=np.vstack(hiphop_nonhiphop[\"lyrics_embeddings\"].values)\n",
    "y_lyrics2=hiphop_nonhiphop[\"Cluster\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_audio2 = kmeans.fit_predict(X_audio2)\n",
    "ari_audio2 = adjusted_rand_score(y_audio2, y_pred_audio2)\n",
    "print(\"Adjusted Rand Index (ARI) for audio embeddings:\", ari_audio2)\n",
    "y_pred_lyrics2=kmeans.fit_predict(X_lyrics2)\n",
    "ari_lyrics2 = adjusted_rand_score(y_lyrics2, y_pred_lyrics2)\n",
    "print(\"Adjusted Rand Index (ARI) for lyric embeddings:\", ari_lyrics2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix for audio embeddings hiphop vs country/pop\n",
    "true_labels_audio2 = np.array(y_audio2)       \n",
    "predicted_labels_audio2 = np.array(y_pred_audio2)   \n",
    "mapping = map_clusters(true_labels_audio2, predicted_labels_audio2)\n",
    "print(\"Mapping of predicted to true labels:\", mapping)\n",
    "\n",
    "# Remap predicted labels based on the mapping:\n",
    "mapped_predicted_labels = np.array([mapping.get(label, label) for label in predicted_labels_audio2])\n",
    "print(\"Mapped predicted labels:\", mapped_predicted_labels)\n",
    "\n",
    "# Create confusion matrix with remapped labels\n",
    "cm_mapped = confusion_matrix(true_labels_audio2, mapped_predicted_labels)\n",
    "cm_df = pd.DataFrame(cm_mapped, \n",
    "                     index=[f\"True {i}\" for i in sorted(np.unique(true_labels_audio2))],\n",
    "                     columns=[f\"Pred {i}\" for i in sorted(np.unique(mapped_predicted_labels))])\n",
    "print(\"\\nConfusion Matrix (after mapping):\")\n",
    "print(cm_df)\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels_audio2, mapped_predicted_labels)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matrix for lyrics embeddings hiphop vs countr/pop\n",
    "true_labels_lyrics2 = np.array(y_lyrics2)       \n",
    "predicted_labels_lyrics2 = np.array(y_pred_lyrics2)\n",
    "mapping = map_clusters(true_labels_lyrics2, predicted_labels_lyrics2)\n",
    "print(\"Mapping of predicted to true labels:\", mapping)\n",
    "\n",
    "# Remap predicted labels based on the mapping:\n",
    "mapped_predicted_labels = np.array([mapping.get(label, label) for label in predicted_labels_lyrics2])\n",
    "print(\"Mapped predicted labels:\", mapped_predicted_labels)\n",
    "\n",
    "# Create confusion matrix with remapped labels\n",
    "cm_mapped = confusion_matrix(true_labels_lyrics2, mapped_predicted_labels)\n",
    "cm_df = pd.DataFrame(cm_mapped, \n",
    "                     index=[f\"True {i}\" for i in sorted(np.unique(true_labels_lyrics2))],\n",
    "                     columns=[f\"Pred {i}\" for i in sorted(np.unique(mapped_predicted_labels))])\n",
    "print(\"\\nConfusion Matrix (after mapping):\")\n",
    "print(cm_df)\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels_lyrics2, mapped_predicted_labels)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sad vs Bright\n",
    "X_audio3=sample3[sample3.columns[3:]].values\n",
    "y_audio3=sample3[\"Cluster\"].values\n",
    "X_lyrics3=np.vstack(sad_bright[\"lyrics_embeddings\"].values)\n",
    "y_lyrics3=sad_bright[\"Cluster\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_audio3 = kmeans.fit_predict(X_audio3)\n",
    "ari_audio3 = adjusted_rand_score(y_audio3, y_pred_audio3)\n",
    "print(\"Adjusted Rand Index (ARI) for audio embeddings:\", ari_audio3)\n",
    "y_pred_lyrics3=kmeans.fit_predict(X_lyrics3)\n",
    "ari_lyrics3 = adjusted_rand_score(y_lyrics3, y_pred_lyrics3)\n",
    "print(\"Adjusted Rand Index (ARI) for lyric embeddings:\", ari_lyrics3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scale up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successfully_processed_sample=pd.read_parquet(\"processed_sample.parquet\")\n",
    "# Filter out those we weren't able to extract lyrics\n",
    "with_lyrics=successfully_processed_sample[~successfully_processed_sample['lyrics_embeddings'].apply(lambda arr: all(x == 0 for x in arr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since the BERT-based LLM generates 768-dimensional embeddings while the audio embeddings are 34-dimensional, we need to brigh them down to the same dimension before applying weights and combining them\n",
    "# Preprocess and scale each modality.\n",
    "X_audio = np.vstack(with_lyrics[with_lyrics.columns[2:-3]].values)  # shape: (n_samples, 34)\n",
    "X_lyrics = np.vstack(with_lyrics[with_lyrics.columns[-1]].values)   # shape: (n_samples, 768)\n",
    "\n",
    "scaler_audio = StandardScaler()\n",
    "scaler_lyrics = StandardScaler()\n",
    "X_audio_scaled = scaler_audio.fit_transform(X_audio)\n",
    "X_lyrics_scaled = scaler_lyrics.fit_transform(X_lyrics)\n",
    "\n",
    "X_audio_norm = normalize(X_audio_scaled, norm='l2')\n",
    "X_lyrics_norm = normalize(X_lyrics_scaled, norm='l2')\n",
    "# Step 2: Project each modality to a common latent space.\n",
    "common_dim = 34\n",
    "pca_audio = PCA(n_components=common_dim, random_state=13)\n",
    "pca_lyrics = PCA(n_components=common_dim, random_state=13)\n",
    "X_audio_proj = pca_audio.fit_transform(X_audio_norm)    # shape: (n_samples, common_dim)\n",
    "X_lyrics_proj = pca_lyrics.fit_transform(X_lyrics_norm)    # shape: (n_samples, common_dim)\n",
    "# Step 3: Fuse the embeddings via weighted sum.\n",
    "# Set weights. If you want clustering solely based on lyrics, set lyrics_weight = 1 and audio_weight = 0.\n",
    "lyrics_weight = 0.4   # Predefine this from models' performances on the small samples\n",
    "audio_weight = 1.0 - lyrics_weight\n",
    "\n",
    "def fuse_embeddings(audio_proj, lyric_proj, w_audio, w_lyrics):\n",
    "    return w_audio * audio_proj + w_lyrics * lyric_proj\n",
    "\n",
    "X_fused = fuse_embeddings(X_audio_proj, X_lyrics_proj, audio_weight, lyrics_weight)\n",
    "print(\"Fused embedding shape:\", X_fused.shape)\n",
    "with_lyrics['combined_embedding'] = list(X_fused)\n",
    "# Step 4: Cluster using K-means on the fused embeddings.\n",
    "best_k = None\n",
    "best_score = -1\n",
    "best_labels = None\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=13, n_init=20)\n",
    "    labels = kmeans.fit_predict(X_fused)\n",
    "    score = silhouette_score(X_fused, labels)\n",
    "    print(f\"k = {k}, silhouette score = {score:.4f}\")\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "        best_labels = labels\n",
    "\n",
    "print(f\"\\nBest k: {best_k} with silhouette score: {best_score:.4f}\")\n",
    "# Step 5: PCA visualization (optional, for 2D visualization)\n",
    "plt.figure(figsize=(8,6))\n",
    "# Define boundaries to force two discrete bins: anything below 0.5 maps to 0 and above maps to 1.\n",
    "boundaries = [-0.5, 0.5, 1.5]\n",
    "norm = mpl.colors.BoundaryNorm(boundaries, ncolors=256, clip=True)\n",
    "\n",
    "# Create the scatter plot using the norm.\n",
    "scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=best_labels, cmap='viridis', norm=norm, alpha=0.7)\n",
    "plt.title(\"Clusters visualized in 2-dimensional space\")\n",
    "plt.xlabel(\"The direction in which the data has the largest spread (variance)\")\n",
    "plt.ylabel(\"The next direction of greatest spread, but orthogonal to PC1\")\n",
    "\n",
    "# Create a colorbar with the discrete boundaries.\n",
    "cbar = plt.colorbar(scatter, boundaries=boundaries, ticks=[0, 1])\n",
    "cbar.ax.set_yticklabels(['Cluster 1', 'Cluster 2'])\n",
    "plt.show()\n",
    "\n",
    "# Optionally, compute overall silhouette score\n",
    "overall_silhouette = silhouette_score(X_fused, best_labels)\n",
    "print(f\"Overall silhouette score: {overall_silhouette:.4f}\")\n",
    "\n",
    "# Optionally, compute overall silhouette score\n",
    "overall_silhouette = silhouette_score(X_fused, best_labels)\n",
    "print(f\"Overall silhouette score: {overall_silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Compute cosine similarity within clusters and generate recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intra_cluster_similarity(df, cluster_col='cluster_ensemble', embedding_col='combined_embedding'):\n",
    "    \"\"\"\n",
    "    Compute the average cosine similarity for songs within each cluster and overall.\n",
    "    \n",
    "    Parameters:\n",
    "      df: DataFrame containing cluster labels and embeddings.\n",
    "      cluster_col: Name of the column with cluster labels.\n",
    "      embedding_col: Name of the column with the fused embedding (as a NumPy array).\n",
    "      \n",
    "    Returns:\n",
    "      cluster_similarity: Dictionary mapping each cluster label to its average cosine similarity.\n",
    "      overall_avg_similarity: Average cosine similarity across clusters.\n",
    "    \"\"\"\n",
    "    cluster_similarity = {}\n",
    "    # Loop over each unique cluster.\n",
    "    for cl in df[cluster_col].unique():\n",
    "        # Get all embeddings for this cluster.\n",
    "        cluster_df = df[df[cluster_col] == cl]\n",
    "        embeddings = np.vstack(cluster_df[embedding_col].values)\n",
    "        n_samples = embeddings.shape[0]\n",
    "        if n_samples < 2:\n",
    "            # Cannot compute similarity for a single song; skip or set to NaN.\n",
    "            cluster_similarity[cl] = np.nan\n",
    "            continue\n",
    "        # Compute cosine similarity matrix.\n",
    "        sim_matrix = cosine_similarity(embeddings)\n",
    "        # Exclude the diagonal (self-similarity = 1) by subtracting n_samples.\n",
    "        # Then average over all off-diagonal elements.\n",
    "        total_sim = np.sum(sim_matrix) - n_samples  # subtract diagonal ones.\n",
    "        num_pairs = n_samples * (n_samples - 1)\n",
    "        avg_sim = total_sim / num_pairs\n",
    "        cluster_similarity[cl] = avg_sim\n",
    "\n",
    "    # Compute overall average (ignoring clusters with NaN values)\n",
    "    valid_sims = [sim for sim in cluster_similarity.values() if not np.isnan(sim)]\n",
    "    overall_avg_similarity = np.mean(valid_sims) if valid_sims else np.nan\n",
    "    return cluster_similarity, overall_avg_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sim, overall_sim = compute_intra_cluster_similarity(with_lyrics)\n",
    "print(\"Intra-cluster cosine similarity for each cluster:\")\n",
    "for cl, sim in cluster_sim.items():\n",
    "    print(f\"Cluster {cl}: {sim:.4f}\")\n",
    "print(f\"Overall average intra-cluster cosine similarity: {overall_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_top_10(query_index, df):\n",
    "    \"\"\"\n",
    "    Given a query song index, computes cosine similarity between its combined embedding\n",
    "    (stored in df['combined_embedding']) and all songs, then returns:\n",
    "      - A DataFrame with the top 10 most similar songs (excluding the query),\n",
    "      - The overall similarity (average cosine similarity of these top 10 songs),\n",
    "      - The similarity scores for each recommended song.\n",
    "    \n",
    "    Parameters:\n",
    "      query_index (int): Index of the query song in df.\n",
    "      df (DataFrame): DataFrame with at least a 'combined_embedding' column containing\n",
    "                      the fused embedding (as a NumPy array) and metadata columns like 'title' and 'artist'.\n",
    "    \n",
    "    Returns:\n",
    "      recommendations (DataFrame): Top 10 similar songs with their metadata.\n",
    "      overall_similarity (float): The average cosine similarity of the top 10 songs.\n",
    "      top_similarities (np.array): The cosine similarity scores for the top 10 songs.\n",
    "    \"\"\"\n",
    "    # Extract the query song's embedding and reshape to 2D.\n",
    "    query_embedding = df.loc[query_index, 'combined_embedding'].reshape(1, -1)\n",
    "    \n",
    "    # Stack all combined embeddings into a 2D array.\n",
    "    all_embeddings = np.vstack(df['combined_embedding'].values)\n",
    "    \n",
    "    # Compute cosine similarity between query and all songs.\n",
    "    similarities = cosine_similarity(query_embedding, all_embeddings)[0]\n",
    "    \n",
    "    # Sort indices by similarity in descending order.\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    # Exclude the query song itself.\n",
    "    sorted_indices = sorted_indices[sorted_indices != query_index]\n",
    "    \n",
    "    # Select the top 10 most similar song indices.\n",
    "    top_indices = sorted_indices[:10]\n",
    "    \n",
    "    # Get the similarity scores for these top songs.\n",
    "    top_similarities = similarities[top_indices]\n",
    "    \n",
    "    # Compute overall similarity (average of the top 10 similarities).\n",
    "    overall_similarity = np.mean(top_similarities)\n",
    "    \n",
    "    # Retrieve metadata (e.g., title, artist) for recommended songs.\n",
    "    recommendations = df.iloc[top_indices][['title', 'artist']]\n",
    "    \n",
    "    return recommendations, overall_similarity, top_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "with_lyrics=with_lyrics.reset_index(drop=True)\n",
    "for i in range(0,10):\n",
    "    query_idx=random.randint(0, len(with_lyrics))\n",
    "    recs, overall_sim, sim_scores = recommend_top_10(query_idx, with_lyrics)\n",
    "    print(\"Top 10 recommendations for song index\", query_idx)\n",
    "    print(recs)\n",
    "    print(\"Overall average similarity:\", overall_sim)\n",
    "    print(\"Similarity scores:\", sim_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
